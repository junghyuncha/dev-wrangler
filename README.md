# instructLabì„ ì‚¬ìš©í•˜ì—¬ nmon Wrangler ë§Œì˜ LLM ìƒì„±

## ì°¸ê³  ê°€ì´ë“œ
- ì „ì²´ì ì¸ ê³¼ì •ì€ ì•„ë˜ urlì˜ ê°€ì´ë“œë¥¼ ì°¸ê³  í•˜ì˜€ìŠµë‹ˆë‹¤.
* [https://docs.instructlab.ai/getting-started/linux\_amd/](https://docs.instructlab.ai/getting-started/linux_amd/)
* [https://github.com/instructlab/instructlab](https://github.com/instructlab/instructlab)


## ê°œìš”

* **ì§„í–‰ ê°œìš”** : ë‚˜ë§Œì˜ ë°ì´í„°ë¡œ LLMì„ íŠœë‹í•´ì„œ, ëª©ì ì— ë§ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ë§Œë“œëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸ êµ¬ì„±.

* **ëª©ì ** : nmon sample dataë¥¼ ì‚¬ìš©í•˜ì—¬ input nmon fileì— ëŒ€í•´ ë¶„ì„ ë˜ëŠ” ê°œì„ ì ì„ ì œì•ˆí•´ì£¼ëŠ” ëª¨ë¸ì„ ë§Œë“ ë‹¤. ì´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ nmon Wranglerì— data input ì‹œ ë¶„ì„í•´ì£¼ëŠ” ê¸°ëŠ¥ì„ ì¶”ê°€ í•  ì˜ˆì •.(1ì°¨ í…ŒìŠ¤íŠ¸ëŠ” chatbot í˜•ì‹ìœ¼ë¡œ ì§„í–‰ í•˜ì—¬ tuned ëª¨ë¸ ê²€ì¦ í›„ ìµœì¢… ì™„ì„±ëœ modelì„ ì‚¬ìš©í•˜ì—¬ nmon Wranglerì— enable í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ë‹¤ì‹œ ì°¾ì„ ì˜ˆì •)

* **í•œê³„ì ** : model fine tuning ì‹œ GPU ê°€ ì—†ëŠ” pcë¥¼ ì‚¬ìš©í•  ê²½ìš° ì‹œê°„ì´ ë„ˆë¬´ ë§ì´ ì†Œìš”ë¨.


## ê°œë°œ í™˜ê²½

* Windows WSL
* Ubuntu 22.04
* Python 3.11


## Step 1: ê°€ìƒ í™˜ê²½ ìƒì„±

```bash
python3.11 -m venv myenv
source myenv/bin/activate
```


## Step 2: íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
pip install --upgrade pip setuptools wheel

sudo apt update
sudo apt install -y build-essential cmake
```


## Step 3: instructLab ì„¤ì¹˜

```bash
pip install instructlab
```


## Step 4: instructLab í™˜ê²½ ì´ˆê¸°í™”

```bash
ilab config init
```

* GPU ì—†ëŠ” ê²½ìš° `0` (No Profile) ì„ íƒ
* GPUê°€ ì—†ëŠ” ê²½ìš°, 0 no profile ì„ íƒí•˜ë¼ëŠ” document ë‚´ìš©. 
â€œWhen prompted, please choose a train profile. Train profiles are GPU specific profiles that enable accelerated training behavior. If you are on MacOS or a Linux machine without a dedicated GPU, please chooseÂ No Profile (CPU, Apple Metal, AMD ROCm)Â by hitting Enter. There are various flags you can utilize with individualÂ ilabÂ commands that allow you to utilize your GPU if applicable.â€

```bash
(venv) root@junghyun:~/instructlab# ilab config init
Existing config file was found in:
Â  Â  /root/.config/instructlab/config.yaml
Do you still want to continue? [y/N]: y

Existing system profiles were found in:
Â  Â  /root/.local/share/instructlab/internal/system_profiles
Do you want to restore these profiles to the default values? [y/N]: y

----------------------------------------------------
Â  Â  Â  Â  Â Welcome to the InstructLab CLI
Â  This guide will help you to setup your environment
----------------------------------------------------

Please provide the following values to initiate the environment [press 'Enter' for default options when prompted]
Path to taxonomy repo [/root/.local/share/instructlab/taxonomy]:
Path to your model [/root/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf]:
INFO 2025-05-27 14:22:44,945 instructlab.config.init:64:
Generating config file and profiles:
Â  Â  /root/.config/instructlab/config.yaml
Â  Â  /root/.local/share/instructlab/internal/system_profiles

WARNING 2025-05-27 14:22:47,380 instructlab.config.init:125: ilab is only officially supported on Linux and MacOS with M-Series Chips
Please choose a system profile.
Profiles set hardware-specific defaults for all commands and sections of the configuration.
First, please select the hardware vendor your system falls into
[0] NO SYSTEM PROFILE
[1] APPLE
[2] INTEL
[3] AMD
[4] NVIDIA
Enter the number of your choice [0]: 0
No profile selected - ilab will use generic code defaults - these may not be optimized for your system.

--------------------------------------------
Â  Â  Initialization completed successfully!
Â  You're ready to start using `ilab`. Enjoy!
--------------------------------------------
(venv) root@junghyun:~/instructlab# ilab config show

```


## Step 5: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

```bash
(venv) root@junghyun:~/instructlab# ilab model download
```

* ê¸°ë³¸ìœ¼ë¡œ instructLabì—ì„œ ì‚¬ìš©ë˜ëŠ” ëª¨ë¸ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.
* ì´ ì¤‘ flagë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* ìš°ë¦¬ëŠ” instructë¥¼ ì‚¬ìš© í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— mistral-7b-instruct ë˜ëŠ” granite-7bë¥¼ ì‚¬ìš© í•˜ê³ ì í•©ë‹ˆë‹¤. (7bëŠ” ë„ˆë¬´ ì»¤ì„œ(íŒŒë¼ë¯¸í„° ê°œìˆ˜ê°€ ë„ˆë¬´ ë§ìŒ 70ì–µê°œ) gpuê°€ ì—†ëŠ” í™˜ê²½ì—ì„œ ì‚¬ìš©í•˜ê¸°ì— ë¬´ë¦¬ê°€ ìˆê¸´ í•©ë‹ˆë‹¤... ê·¸ëŸ°ë° ì–¸ì–´ëª¨ë¸ì€ ëª¨ë‘ ì»¤ì„œ.. hugging faceì—ì„œ ì–‘ìí™” ëœ ëª¨ë¸ì„ ì°¾ì•„ ì´ë¥¼ ë‹¤ìš´ë°›ì•„ ì‚¬ìš© í•˜ë©´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì¼ë‹¨ í…ŒìŠ¤íŠ¸ëŠ” mistral-7bë¡œ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.)

```bash
(venv) root@junghyun:~/instructlab# ilab model download
INFO 2025-05-27 14:53:33,080 instructlab.model.download:302: Available models (`ilab model list`):
+--------------------------------------------+---------------------+----------+----------------------------------------------------------------------+
| Model Name Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  | Last Modified Â  Â  Â  | Size Â  Â  | Absolute path Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â |
+--------------------------------------------+---------------------+----------+----------------------------------------------------------------------+
| mistral-7b-instruct-v0.2.Q4_K_M.gguf Â  Â  Â  | 2025-05-27 14:53:14 | 4.1 GB Â  | /root/.cache/instructlab/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf |
| merlinite-7b-lab-Q4_K_M.gguf Â  Â  Â  Â  Â  Â  Â  | 2025-05-27 14:51:59 | 4.1 GB Â  | /root/.cache/instructlab/models/merlinite-7b-lab-Q4_K_M.gguf Â  Â  Â  Â  |
| ibm-granite/granite-embedding-125m-english | 2025-05-27 14:53:33 | 479.2 MB | /root/.cache/instructlab/models/ibm-granite Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â |
| granite-7b-lab-Q4_K_M.gguf Â  Â  Â  Â  Â  Â  Â  Â  | 2025-05-27 14:50:42 | 3.8 GB Â  | /root/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf Â  Â  Â  Â  Â  |
+--------------------------------------------
```


## Step 6: ëª¨ë¸ ì„œë¹™ ë° Chat Test

ëª¨ë¸ ì„ íƒ. (granite-7b-lab-Q4_K_M.gguf)
```bash
(venv) root@junghyun:~/instructlab# ilab model serve --model-path /root/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf
INFO 2025-05-27 15:35:12,095 instructlab.model.serve_backend:80: Setting backend_type in the serve config to llama-cpp
INFO 2025-05-27 15:35:12,108 instructlab.model.serve_backend:86: Using model '/root/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf' with -1 gpu-layers and 4096 max context size.llama_new_context_with_model: n_ctx_pre_seq (4096) > n_ctx_train (2048) -- possible training context overflowINFO 2025-05-27 15:35:20,380 instructlab.model.backends.llama_cpp:306: Replacing chat template: {% set eos_token = "<|endoftext|>" %}{% set bos_token = "<|begginingoftext|>" %}{% for message in messages %}{% if message['role'] == 'pretraining' %}{{'<|pretrain|>' + message['content'] + '<|endoftext|>' + '<|/pretrain|>' }}{% elif message['role'] == 'system' %}{{'<|system|>'+ '' + message['content'] + ''}}{% elif message['role'] == 'user' %}{{'<|user|>' + '' + message['content'] + ''}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>' + '' + message['content'] + '<|endoftext|>' + ('' if loop.last else '')}}{% endif %}{% if loop.last and add_generation_prompt %}{{ '<|assistant|>' + '' }}{% endif %}{% endfor %}
INFO 2025-05-27 15:35:20,388 instructlab.model.backends.llama_cpp:233: Starting server process, press CTRL+C to shutdown server...INFO 2025-05-27 15:35:20,389 instructlab.model.backends.llama_cpp:234: After application startup complete seeÂ http://127.0.0.1:8000/docsÂ for API.
```

------------ìƒˆë¡œìš´ í„°ë¯¸ë„ ì˜¤í”ˆ------------

```bash
wsl
```
```bash
sudo -i
```
```bash
python3 -m venv venv
```
```bash
source venv/bin/activate
```

```bash
(venv) root@junghyun:~# ilab model chat

â•­â”€â”€â”€ system  Welcome to InstructLab Chat : GRANITE-7B-LAB-Q4_K_M.GGUF (type /h for help) 

Q : what is the capital of south Korea? [S][default]

granite-7b-lab-Q4_K_M.gguf 

The capital city of South Korea is Seoul, a vibrant metropolis known for its rich history, bustling culture, and technological advancements. If you have any questions about Seoul or other parts of South Korea, I'd be happy to help! 
â”€â”€â”€ elapsed 10.470 seconds â”€â•¯>>>
```

->í•œêµ­ì˜ ìˆ˜ë„ë¥¼ ì˜ ì•Œë ¤ì¤ë‹ˆë‹¤.



## Step 7: instruction yaml êµ¬ì„± ë° GitHub ì—…ë¡œë“œ

* instructLabì€ taxonomyë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì´ í•™ìŠµ í•˜ê²Œ ë  ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. (taxonomyëŠ” ëª¨ë¸ì´ í•™ìŠµí•˜ê²Œ ë  ì§€ì‹, ê¸°ëŠ¥, ê°œë…ë“¤ì„ ê³„ì¸µì ìœ¼ë¡œ êµ¬ì¡°í™”í•œ ë¶„ë¥˜ ì²´ê³„ ì…ë‹ˆë‹¤.)
* ë‹¤ìŒ ê³¼ì •ì—ì„œ ë³´ì‹œê² ì§€ë§Œ ilab data generate ê³¼ì •ì„ ê±°ì³ .jsonl íŒŒì¼ì„ ìƒì„±í•˜ëŠ”ë° ì´ë•Œ ~/.local/share/instructlab/taxonomy ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ ìë™ ìŠ¤ìº”í•©ë‹ˆë‹¤. ê·¸ë˜ì„œ ìƒˆë¡œìš´ .yaml íŒŒì¼ì´ ìˆëŠ”ì§€ ê²€í†  í•©ë‹ˆë‹¤.
* ì´ instruction yaml íŒŒì¼ì€ ë‹¨ìˆœí•œ ë°ì´í„° íŒŒì¼ì´ ì•„ë‹ˆë¼ taxonomyì˜ leaf nodeì—­í• ì„ í•©ë‹ˆë‹¤. ê·¸ë˜ì„œ íŒŒì¼ì„ ì‘ì„±í•˜ëŠ”ë° ë‚˜ë¦„ì˜ ê·œì¹™ë„ ìˆê³  ê¼­ ë“¤ì–´ ê°€ì•¼ í•˜ëŠ” í•­ëª©ì´ ì¡´ì¬í•©ë‹ˆë‹¤.
* ì €ëŠ” nmon data ì¤‘ memuse í•­ëª©ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ êµ¬ì„± í•˜ì˜€ìŠµë‹ˆë‹¤.

### step 7-1: git ì— repository ìƒì„± í›„ .md íŒŒì¼ ìƒì„±
https://github.com/junghyuncha/instructlab/blob/main/nmon_memuse.md

* memuse í•­ëª©ì— ëŒ€í•œ ì„¤ëª…ê³¼ ê°„ë‹¨í•œ ì˜ˆì‹œ, ë¶„ì„ ë‚´ìš©ì´ ë‹´ê¸´ ë‚´ìš©ì…ë‹ˆë‹¤.
* ì €ëŠ” memuseì— ëŒ€í•´ ì •ë¦¬ í•˜ì˜€ì§€ë§Œ cpuuse, page ë“± í˜„ì¬ ìš°ë¦¬ê°€ ìˆ˜ì§‘í•˜ëŠ” nmon dataì— ëŒ€í•´ ê°ê° ìƒì„±í•˜ì—¬ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤.(ëª¨ë¸ì˜ í•™ìŠµì„ ìœ„í•´)

### step 7-2: yaml file ì‘ì„±
* ì´ yaml fileì„ ì‚¬ìš©í•˜ì—¬ .jsonl íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤. (model trainì„ ì´ .jsonl íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ ì§„í–‰í•˜ê¸° ë•Œë¬¸)
* ì €ëŠ” nmonWrangler ë„ë©”ì¸ì„ ì‚¬ìš©í•˜ì˜€ê³  íŒŒì¼ ìœ„ì¹˜ëŠ” /root/.local/share/instructlab/taxonomy/knowledge/nmonWrangler/collections/memuse/qna.yaml ë¡œ ì§€ì • í•˜ì˜€ìŠµë‹ˆë‹¤.
* íŒŒì¼ ì‘ì„±ì—ëŠ” ì—¬ëŸ¬ ê·œì¹™ì´ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ í•­ëª©ì´ í•„ë“œ êµ¬ë¬¸ì— ë§ê²Œ ë“¤ì–´ê°€ì•¼ í•©ë‹ˆë‹¤.
    * domain, created_by, document, seed_examples, context, questions_and_answers, repo(githubì£¼ì†Œ), commit(commit id), patterns(yaml íŒŒì¼ ìœ„ì¹˜) 
    * seed_examples ë¥¼ ìµœì†Œ 5ê°œ ì‘ì„±
    * ê° questions_and_answers ë¸”ë¡ì— ì§ˆë¬¸ 3ê°œ ì´ìƒ ì‘ì„±

[yaml íŒŒì¼ì˜ ì¼ë¶€]

```yaml
version: 3
domain: nmonWrangler
created_by: junghyun
seed_examples:
Â  - context: |
Â  Â  Â  The following is a snapshot of the `memuse` section from an AIX system collected using nmon:
Â  Â  Â  - %numperm: 91.2
Â  Â  Â  - %minperm: 3.0
Â  Â  Â  - %maxperm: 90.0
Â  Â  Â  - %numclient: 92.5
Â  Â  Â  - %maxclient: 90.0
Â  Â  Â  - minfree: 960
Â  Â  Â  - maxfree: 1088
Â  Â  Â  - lruable pages: 130171072

Â  Â  Â  These metrics reflect the systemâ€™s memory usage. The `perm` values indicate file system cache, while `client` values relate to application cache.
Â  Â  questions_and_answers:
Â  Â  Â  - question: |
Â  Â  Â  Â  Â  What problems are indicated by this memuse data?
Â  Â  Â  Â  answer: |
Â  Â  Â  Â  Â  Both %numperm and %numclient exceed their respective %maxperm and %maxclient thresholds. This suggests that the file system and applications are overusing cache memory, which could lead to page stealing and system performance degradation. You should consider tuning cache settings or increasing physical memory.

Â  Â  Â  - question: |
Â  Â  Â  Â  Â  What does the %numperm field represent?
Â  Â  Â  Â  answer: |
Â  Â  Â  Â  Â  %numperm indicates the percentage of memory used by the persistent file system cache. It is recommended to keep this value below %maxperm to avoid I/O bottlenecks and page stealing issues.

Â  Â  Â  - question: |
Â  Â  Â  Â  Â  What configuration or tuning can be done to improve this?
Â  Â  Â  Â  answer: |
Â  Â  Â  Â  Â  Use the `vmo` command to adjust maxperm and maxclient thresholds. Additionally, tuning `minfree` and `maxfree` can help ensure adequate free pages. Itâ€™s also advisable to review application memory usage and minimize unnecessary cache retention.

# ë¹„ìŠ·í•˜ê²Œ context 4ê°œ ë” ì¶”ê°€

document:
  repo: https://github.com/junghyuncha/instructlab
  commit: cdb021ba10aa638c8f91c746aad8bc9761ee2073
  patterns:
    - nmon_memuse.md
```

### 7-3: yaml íŒŒì¼ ê²€í† 
* íŒŒì¼ ì‘ì„±ì„ ì™„ë£Œí•˜ë©´ ilab taxonomy diff ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€í† í•©ë‹ˆë‹¤.
* íŒŒì¼ ì‘ì„±ì— ë¬¸ì œê°€ ì—†ë‹¤ë©´ ì•„ë˜ì™€ ê°™ì´ ë¡œê·¸ê°€ ì¶œë ¥ë©ë‹ˆë‹¤.
```bash
(venv) root@junghyun:~/instructlab# ilab taxonomy diff
```

ê²°ê³¼ ì˜ˆ:

```
Taxonomy in /root/.local/share/instructlab/taxonomy is valid :)
```


## Step 8: .jsonl í•™ìŠµ ë°ì´í„° ìƒì„±

* ëª¨ë¸ì„ train ì‹œí‚¬ë•Œ ì‚¬ìš©í•˜ëŠ” .jsonl íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤. 
* ìœ„ì—ì„œ ë§Œë“¤ì–´ ë†“ì€ yamlì„ ì°¸ê³ í•˜ì—¬ ìƒì„± í•˜ê²Œ ë©ë‹ˆë‹¤.
* ì‹œê°„ì´ ì¡°ê¸ˆ ì†Œìš”ë©ë‹ˆë‹¤.. ì €ì˜ ê²½ìš° ilab config init ì‹œ gpuê°€ ì—†ëŠ” í™˜ê²½ì¸ 0 no profileì„ ì„ íƒ í•˜ì˜€ê¸° ë•Œë¬¸ì— cpuë¡œ ì´ ë°ì´í„°ë¥¼ ë§Œë“œëŠ”ë° 18ì‹œê°„ ì •ë„ ê±¸ë ¸ìŠµë‹ˆë‹¤.

```bash
(venv) root@junghyun:~/.local/share/instructlab/taxonomy/knowledge/nmonWrangler/collections/memuse# ilab data generate
INFO 2025-05-27 19:55:32,321 instructlab.process.process:300: Started subprocess with PID 9860. Logs are being written to /root/.local/share/instructlab/logs/generation/generation-1c306fba-3ae9-11f0-b0dd-0b4567577773.log.
INFO 2025-05-27 19:55:32,622 instructlab.model.backends.llama_cpp:126: Trying to connect to model server at http://127.0.0.1:8000/v1
llama_new_context_with_model: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
WARNING 2025-05-27 19:55:43,037 instructlab:188: Disabling SDG batching - unsupported with llama.cpp serving
INFO 2025-05-27 19:55:43,271 numexpr.utils:162: NumExpr defaulting to 12 threads.
INFO 2025-05-27 19:55:43,561 datasets:54: PyTorch version 2.7.0 available.
INFO 2025-05-27 19:55:43,965 instructlab:206: Generating synthetic data using 'full' pipeline, '/root/.cache/instructlab/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf' model, '/root/.local/share/instructlab/taxonomy' taxonomy, against http://127.0.0.1:38241/v1 server
INFO 2025-05-27 19:55:43,966 root:356: Converting taxonomy to samples
INFO 2025-05-27 19:55:44,791 instructlab.sdg.utils.taxonomy:143: Processing files...
INFO 2025-05-27 19:55:44,791 instructlab.sdg.utils.taxonomy:148: Pattern 'nmon_memuse.md' matched 1 files.
INFO 2025-05-27 19:55:44,791 instructlab.sdg.utils.taxonomy:152: Processing file: /root/.local/share/instructlab/datasets/2025-05-27_195532/preprocessed_2025-05-27T19_55_43/documents/knowledge_nmonWrangler_collections_memuse_u5hs49g3/nmon_memuse.md
INFO 2025-05-27 19:55:44,791 instructlab.sdg.utils.taxonomy:156: Added file path: /root/.local/share/instructlab/datasets/2025-05-27_195532/preprocessed_2025-05-27T19_55_43/documents/knowledge_nmonWrangler_collections_memuse_u5hs49g3/nmon_memuse.md
INFO 2025-05-27 19:55:48,807 instructlab.sdg.utils.chunkers:141: Docling models not found on disk, downloading models...
INFO 2025-05-27 19:55:48,808 docling.utils.model_downloader:45: Downloading layout model...
INFO 2025-05-27 19:56:03,965 docling.utils.model_downloader:53: Downloading tableformer model...
INFO 2025-05-27 19:56:10,752 docling.utils.model_downloader:61: Downloading picture classifier model...
INFO 2025-05-27 19:56:12,811 docling.utils.model_downloader:69: Downloading code formula model...
INFO 2025-05-27 19:56:29,439 docling.utils.model_downloader:114: Downloading easyocr models...
WARNING 2025-05-27 19:56:35,156 instructlab.sdg.utils.chunkers:60: Tesseract not found, falling back to EasyOCR.
INFO 2025-05-27 19:56:35,163 docling.utils.accelerator_utils:67: Accelerator device: 'cpu'
WARNING 2025-05-27 19:56:35,166 easyocr.easyocr:251: Downloading detection model, please wait. This may take several minutes depending upon your network connection.
INFO 2025-05-27 19:56:38,534 easyocr.easyocr:255: Download complete
WARNING 2025-05-27 19:56:38,534 easyocr.easyocr:176: Downloading recognition model, please wait. This may take several minutes depending upon your network connection.
INFO 2025-05-27 19:56:40,172 easyocr.easyocr:180: Download complete.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Merges were not in checkpoint, building merges on the fly.
100%|##########| 32000/32000 [00:20<00:00, 1564.48it/s]
INFO 2025-05-27 19:57:04,583 instructlab.sdg.utils.chunkers:249: Successfully loaded tokenizer from: /root/.cache/instructlab/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf
INFO 2025-05-27 19:57:04,625 docling.document_converter:271: Going to convert document batch...
INFO 2025-05-27 19:57:04,625 docling.document_converter:306: Initializing pipeline for SimplePipeline with options hash 4cc01982ae99b46a2a63fcda46c47c35
INFO 2025-05-27 19:57:04,625 docling.pipeline.base_pipeline:39: Processing document nmon_memuse.md
INFO 2025-05-27 19:57:04,671 docling.document_converter:286: Finished converting document nmon_memuse.md in 0.05 sec.
INFO 2025-05-27 19:57:04,728 instructlab.sdg.generate_data:405: Taxonomy converted to samples and written to /root/.local/share/instructlab/datasets/2025-05-27_195532/preprocessed_2025-05-27T19_55_43
INFO 2025-05-27 19:57:04,739 instructlab.sdg.generate_data:441: Synthesizing new instructions. If you aren't satisfied with the generated instructions, interrupt training (Ctrl-C) and try adjusting your YAML files. Adding more examples may help.
INFO 2025-05-27 19:57:05,414 instructlab.sdg.checkpointing:59: No existing checkpoints found in /root/.local/share/instructlab/datasets/checkpoints/knowledge_nmonWrangler_collections_memuse, generating from scratch
INFO 2025-05-27 19:57:05,414 instructlab.sdg.pipeline:156: Running pipeline single-threaded
INFO 2025-05-27 19:57:05,414 instructlab.sdg.pipeline:199: Running block: duplicate_document_col
INFO 2025-05-27 19:57:05,414 instructlab.sdg.pipeline:203: Batching disabled; processing block 'duplicate_document_col' single-threaded.
INFO 2025-05-27 19:57:05,422 instructlab.sdg.blocks.llmblock:56: LLM server supports batched inputs: False
INFO 2025-05-27 19:57:05,422 instructlab.sdg.pipeline:199: Running block: gen_spellcheck
INFO 2025-05-27 19:57:05,422 instructlab.sdg.pipeline:203: Batching disabled; processing block 'gen_spellcheck' single-threaded.
gen_spellcheck Prompt Generation:   0%|          | 0/35 [00:00<?, ?it/s]/root/myenv/lib/python3.11/site-packages/llama_cpp/llama.py:1237: RuntimeWarning: Detected duplicate leading "<s>" in prompt, this will likely reduce response quality, consider removing it...
  warnings.warn(
gen_spellcheck Prompt Generation:  23%|##2       | 8/35 [03:35<12:04, 26.83s/it]



Filter (num_proc=8): 100%|##########| 453/453 [00:00<00:00, 2176.96 examples/s]
INFO 2025-05-28 10:26:20,576 instructlab.sdg.pipeline:199: Running block: eval_relevancy_qa_pair
INFO 2025-05-28 10:26:20,576 instructlab.sdg.pipeline:203: Batching disabled; processing block 'eval_relevancy_qa_pair' single-threaded.
eval_relevancy_qa_pair Prompt Generation: 100%|##########| 158/158 [1:12:17<00:00, 27.45s/it]
INFO 2025-05-28 11:38:38,265 instructlab.sdg.pipeline:199: Running block: filter_relevancy
INFO 2025-05-28 11:38:38,266 instructlab.sdg.pipeline:203: Batching disabled; processing block 'filter_relevancy' single-threaded.
Map (num_proc=8): 100%|##########| 158/158 [00:00<00:00, 556.93 examples/s]
Filter (num_proc=8): 100%|##########| 158/158 [00:00<00:00, 952.41 examples/s]
INFO 2025-05-28 11:38:39,052 instructlab.sdg.pipeline:199: Running block: eval_verify_question
INFO 2025-05-28 11:38:39,052 instructlab.sdg.pipeline:203: Batching disabled; processing block 'eval_verify_question' single-threaded.
eval_verify_question Prompt Generation: 100%|##########| 155/155 [1:02:47<00:00, 24.31s/it]
INFO 2025-05-28 12:41:27,144 instructlab.sdg.pipeline:199: Running block: filter_verify_question
INFO 2025-05-28 12:41:27,146 instructlab.sdg.pipeline:203: Batching disabled; processing block 'filter_verify_question' single-threaded.
Map (num_proc=8): 100%|##########| 149/149 [00:00<00:00, 547.72 examples/s]
Filter (num_proc=8): 100%|##########| 149/149 [00:00<00:00, 760.13 examples/s]
INFO 2025-05-28 12:41:27,965 instructlab.sdg.generate_data:478: Generated 134 samples
INFO 2025-05-28 12:41:28,029 instructlab.sdg.pipeline:156: Running pipeline single-threaded
INFO 2025-05-28 12:41:28,041 instructlab.sdg.pipeline:199: Running block: gen_mmlu_knowledge
INFO 2025-05-28 12:41:28,041 instructlab.sdg.pipeline:203: Batching disabled; processing block 'gen_mmlu_knowledge' single-threaded.
gen_mmlu_knowledge Prompt Generation: 100%|##########| 35/35 [31:42<00:00, 54.36s/it]
Filter: 100%|##########| 13/13 [00:00<00:00, 455.87 examples/s]
Filter: 100%|##########| 10/10 [00:00<00:00, 473.17 examples/s]
Flattening the indices: 100%|##########| 10/10 [00:00<00:00, 1688.12 examples/s]
Map: 100%|##########| 10/10 [00:00<00:00, 2463.61 examples/s]
Map: 100%|##########| 10/10 [00:00<00:00, 671.91 examples/s]
Map: 100%|##########| 10/10 [00:00<00:00, 2309.64 examples/s]
Filter: 100%|##########| 10/10 [00:00<00:00, 2248.72 examples/s]
Filter: 100%|##########| 10/10 [00:00<00:00, 4698.97 examples/s]
Filter: 100%|##########| 9/9 [00:00<00:00, 4457.28 examples/s]
Flattening the indices: 100%|##########| 9/9 [00:00<00:00, 2173.21 examples/s]
Casting to class labels: 100%|##########| 9/9 [00:00<00:00, 799.24 examples/s]
INFO 2025-05-28 13:13:10,878 instructlab.sdg.eval_data:126: Saving MMLU Dataset /root/.local/share/instructlab/datasets/2025-05-27_195532/node_datasets_2025-05-27T19_55_43/mmlubench_knowledge_nmonWrangler_collections_memuse.jsonl
Creating json from Arrow format: 100%|##########| 1/1 [00:00<00:00, 32.45ba/s]
INFO 2025-05-28 13:13:10,965 instructlab.sdg.eval_data:130: Saving MMLU Task yaml /root/.local/share/instructlab/datasets/2025-05-27_195532/node_datasets_2025-05-27T19_55_43/knowledge_nmonWrangler_collections_memuse_task.yaml
Map: 100%|##########| 134/134 [00:00<00:00, 4157.88 examples/s]
Map: 100%|##########| 134/134 [00:00<00:00, 12437.19 examples/s]
Filter: 100%|##########| 134/134 [00:00<00:00, 18549.07 examples/s]
Map: 100%|##########| 12/12 [00:00<00:00, 4207.28 examples/s]
Map: 100%|##########| 12/12 [00:00<00:00, 3835.96 examples/s]
Creating json from Arrow format: 100%|##########| 1/1 [00:00<00:00, 134.98ba/s]
Map: 100%|##########| 134/134 [00:00<00:00, 9055.47 examples/s]
Map: 100%|##########| 134/134 [00:00<00:00, 5589.40 examples/s]
Map: 100%|##########| 134/134 [00:00<00:00, 9178.81 examples/s]
Map: 100%|##########| 134/134 [00:00<00:00, 33150.69 examples/s]
Filter: 100%|##########| 134/134 [00:00<00:00, 37697.82 examples/s]
Map: 100%|##########| 12/12 [00:00<00:00, 5622.39 examples/s]
Creating json from Arrow format: 100%|##########| 1/1 [00:00<00:00, 52.31ba/s]
INFO 2025-05-28 13:13:11,258 instructlab.sdg.datamixing:158: Loading dataset from /root/.local/share/instructlab/datasets/2025-05-27_195532/node_datasets_2025-05-27T19_55_43/knowledge_nmonWrangler_collections_memuse_p10.jsonl ...
Generating train split: 280 examples [00:00, 9201.78 examples/s]
INFO 2025-05-28 13:13:12,308 instructlab.sdg.datamixing:160: Dataset columns: ['messages', 'metadata', 'id', 'unmask']
INFO 2025-05-28 13:13:12,308 instructlab.sdg.datamixing:161: Dataset loaded with 280 samples
Map (num_proc=8): 100%|##########| 280/280 [00:00<00:00, 1688.90 examples/s]
Map (num_proc=8): 100%|##########| 280/280 [00:00<00:00, 1702.51 examples/s]
Creating json from Arrow format: 100%|##########| 1/1 [00:00<00:00, 122.59ba/s]
INFO 2025-05-28 13:13:12,968 instructlab.sdg.datamixing:235: Mixed Dataset saved to /root/.local/share/instructlab/datasets/2025-05-27_195532/skills_train_msgs_2025-05-27T19_55_43.jsonl
INFO 2025-05-28 13:13:12,970 instructlab.sdg.datamixing:158: Loading dataset from /root/.local/share/instructlab/datasets/2025-05-27_195532/node_datasets_2025-05-27T19_55_43/knowledge_nmonWrangler_collections_memuse_p07.jsonl ...
Generating train split: 146 examples [00:00, 44377.74 examples/s]
INFO 2025-05-28 13:13:13,187 instructlab.sdg.datamixing:160: Dataset columns: ['messages', 'metadata', 'id', 'unmask']
INFO 2025-05-28 13:13:13,187 instructlab.sdg.datamixing:161: Dataset loaded with 146 samples
Map (num_proc=8): 100%|##########| 146/146 [00:00<00:00, 1136.92 examples/s]
Map (num_proc=8): 100%|##########| 146/146 [00:00<00:00, 1020.67 examples/s]
Creating json from Arrow format: 100%|##########| 1/1 [00:00<00:00, 246.45ba/s]
INFO 2025-05-28 13:13:13,659 instructlab.sdg.datamixing:235: Mixed Dataset saved to /root/.local/share/instructlab/datasets/2025-05-27_195532/knowledge_train_msgs_2025-05-27T19_55_43.jsonl
INFO 2025-05-28 13:13:13,659 instructlab.sdg.generate_data:757: Generation took 62249.69s
á•¦(Ã²á´—Ã³Ë‡)á•¤ Data generate completed successfully! á•¦(Ã²á´—Ã³Ë‡)á•¤

```

## Step 9: ëª¨ë¸ Fine-tuning
* ì‹¤ì œ ìš°ë¦¬ì˜ memuse ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ Fine-tuning í•˜ëŠ” ê³¼ì •.
* ì‹œê°„ì´ ê°€ì¥ ì˜¤ë˜ ê±¸ë¦¬ëŠ” ì‘ì—….
* ìœ„ì˜ ê³¼ì •ì—ì„œ .jsonl íŒŒì¼ì´ ë‘ ê°œê°€ ë§Œë“¤ì–´ ì ¸ì„œ ì´ íŒŒì¼ì„ í•©ì³ì„œ train í•˜ëŠ”ë° ì‚¬ìš© í•˜ê¸°ë¡œ ê²°ì •.


ìƒì„±ëœ ë°ì´í„° íŒŒì¼


skills_train_msgs_*.jsonl	: ì¼ë°˜ì ì¸ Q&A í˜•ì‹ì˜ skills ë°ì´í„°.


knowledge_train_msgs_*.jsonl : knowledge/instruction ìœ í˜•ìœ¼ë¡œ ì¬êµ¬ì„±ëœ í•™ìŠµ ë°ì´í„°.


```bash
(venv) root@junghyun:~/.local/share/instructlab/datasets/2025-05-27_195532# cat knowledge_train_msgs_2025-05-27T19_55_43.jsonl skills_train_msgs_2025-05-27T19_55_43.jsonl > combined_train.jsonl
(venv) root@junghyun:~/.local/share/instructlab/datasets/2025-05-27_195532# pwd
/root/.local/share/instructlab/datasets/2025-05-27_195532
(venv) root@junghyun:~/.local/share/instructlab/datasets/2025-05-27_195532# ll
total 2260
drwxr-xr-x 5 root root Â  4096 May 28 13:20 ./
drwxr-xr-x 4 root root Â  4096 May 27 19:55 ../
-rw-r--r-- 1 root root 992549 May 28 13:20 combined_train.jsonl
drwxr-xr-x 2 root root Â  4096 May 28 12:41 generated_2025-05-27T19_55_43/
-rw-r--r-- 1 root root Â  Â 476 May 28 13:13 knowledge_recipe_2025-05-27T19_55_43.yaml
-rw-r--r-- 1 root root 286892 May 28 13:13 knowledge_train_msgs_2025-05-27T19_55_43.jsonl
-rw-r--r-- 1 root root 103986 May 28 13:13 messages_2025-05-27T19_55_43.jsonl
drwxr-xr-x 2 root root Â  4096 May 28 13:13 node_datasets_2025-05-27T19_55_43/
drwxr-xr-x 3 root root Â  4096 May 27 19:57 preprocessed_2025-05-27T19_55_43/
-rw-r--r-- 1 root root Â  Â 476 May 28 13:13 skills_recipe_2025-05-27T19_55_43.yaml
-rw-r--r-- 1 root root 705657 May 28 13:13 skills_train_msgs_2025-05-27T19_55_43.jsonl
-rw-r--r-- 1 root root Â 86072 May 27 19:57 test_2025-05-27T19_55_43.jsonl
-rw-r--r-- 1 root root Â 93802 May 28 13:13 train_2025-05-27T19_55_43.jsonl
(venv) root@junghyun:~/.local/share/instructlab/datasets/2025-05-27_195532# ilab model train --data-path /root/.local/share/instructlab/datasets/2025-05-27_195532/combined_train.jsonl
```
ì§„í–‰í•˜ë‹¤ê°€ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë ¤ì„œ ì¤‘ê°„ì— interrupt í•˜ì˜€ìŠµë‹ˆë‹¤..


## ì •ë¦¬

ìœ„ ê³¼ì •ì„ í†µí•´ memuse ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ nmon ë¶„ì„ íŠ¹í™” LLMì„ ìƒì„±í•  ì˜ˆì •ì´ë©°, ì´ ëª¨ë¸ì„ nmon Wranglerì— í†µí•©í•˜ì—¬ ì‹¤ ì‚¬ìš©ì— ì í•©í•œ ë¶„ì„ ê¸°ëŠ¥ì„ ì œê³µí•  ê³„íšì…ë‹ˆë‹¤.

> ì¶”ê°€ ì‘ì—…: `cpuuse`, `page`, ê¸°íƒ€ nmon í•­ëª©ì— ëŒ€í•œ yaml ë° ë¬¸ì„œë„ ë³„ë„ë¡œ ì‘ì„± í•„ìš”

## ì´í›„ ê³¼ì •

ì£¼ìš” ë‹¨ê³„ë³„ ì„¤ëª…

1. ilab data generate
2. ilab model train
3. ilab model convert
4. ilab model serve


1ï¸âƒ£ ilab data generate
ëª©ì  :Â taxonomy ê¸°ë°˜ YAML íŒŒì¼ì„ ë¶„ì„í•´ í•™ìŠµìš© .jsonl ë°ì´í„° ìƒì„±
ì´ ë‹¨ê³„ì—ì„œ í•˜ëŠ” ì¼ : taxonomy ì•ˆì˜ seed_examples, Q&A ë“± ë¶„ì„,Â instruction-tuned í˜•ì‹ìœ¼ë¡œ ì¬êµ¬ì„±, jsonl í¬ë§·ì˜ í•™ìŠµ ë°ì´í„° ìƒì„± (ì˜ˆ: knowledge_train_msgs_*.jsonl)
ê²°ê³¼:
/root/.local/share/instructlab/datasets/2025-05-27_XXXXXX/*.jsonl ìƒì„±


2ï¸âƒ£ ilab model train
ëª©ì  : ì•ì—ì„œ ìƒì„±ëœ .jsonl ë°ì´í„°ë¥¼ ì´ìš©í•´ LLMì„ ë¯¸ì„¸ ì¡°ì • (fine-tuning)
ì´ ë‹¨ê³„ì—ì„œ í•˜ëŠ” ì¼ : ì„ íƒí•œ ëª¨ë¸ (ì˜ˆ: mistral, granite ë“±)ì„ ê¸°ë°˜ìœ¼ë¡œ .jsonl ë‚´ ë¬¸ì¥ë“¤ì„ í•™ìŠµ -> ìƒˆë¡œìš´ ì²´í¬í¬ì¸íŠ¸ ìƒì„±
ê²°ê³¼:
checkpoints/instructlab-<model-name>/...ì´ ë‹¨ê³„ëŠ” ê°€ì¥ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ëŠ” ë¶€ë¶„


3ï¸âƒ£ ilab model convert
ëª©ì :í•™ìŠµëœ ëª¨ë¸ì„ .gguf í¬ë§·ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì„œë¹™ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë§Œë“¤ê¸°
ì´ ë‹¨ê³„ì—ì„œ í•˜ëŠ” ì¼:checkpoints/ì— ì €ì¥ëœ ëª¨ë¸ íŒŒì¼ë“¤ì„ llama.cppê°€ ì½ì„ ìˆ˜ ìˆëŠ” .gguf í¬ë§·ìœ¼ë¡œ ë³€í™˜
ê²°ê³¼:
instructlab-<model-name>-trained/<model-name>.gguf ìƒì„±


4ï¸âƒ£ ilab model serveëª©ì : ë³€í™˜ëœ .gguf ëª¨ë¸ì„ ë¡œì»¬ API ì„œë²„ë¡œ ì‹¤í–‰
ì´ ë‹¨ê³„ì—ì„œ í•˜ëŠ” ì¼ : llama.cpp ì„œë²„ë¥¼ ì‹¤í–‰,Â /v1/completions ê°™ì€ OpenAI API í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤ ì œê³µ
ê²°ê³¼:
localhost:8000/v1/â†’Â ì´ì œ ë‚˜ë§Œì˜ fine-tuned LLMì— API ìš”ì²­ì„ ë‚ ë¦´ ìˆ˜ ìˆê²Œ ë¨.


ğŸ”¹ìš”ì•½Â 


ilab data generateÂ 
ì—­í•  : í•™ìŠµìš©Â JSONL ìƒì„±
ìƒì„±ë¬¼ : .jsonl


ilab model train
ì—­í•  :ëª¨ë¸ ë¯¸ì„¸ ì¡°ì • (Fine-tuning)
ìƒì„±ë¬¼ :Â checkpoints/


ilab model convert
ì—­í•  : .gguf í¬ë§·ìœ¼ë¡œ ë³€í™˜
ìƒì„±ë¬¼ :Â *.gguf


ilab model serve
ì—­í•  : APIë¡œ ëª¨ë¸ ì‹¤í–‰
ìƒì„±ë¬¼ : OpenAI í˜¸í™˜ ì„œë²„